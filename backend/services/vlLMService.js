const { HfInference } = require('@huggingface/inference');

/**
 * VL LM (Vision Language Model) Service
 * Fast backup LLM using Hugging Face Vision-Language models
 */
class VLLMService {
    constructor() {
        const apiKey = process.env.HUGGINGFACE_API_KEY || process.env.VLLM_API_KEY;
        
        if (apiKey && apiKey !== 'your-huggingface-api-key-here' && apiKey !== 'your-vllm-api-key-here') {
            this.hf = new HfInference(apiKey);
            this.isAvailable = true;
            // Use a fast vision-language model
            this.model = process.env.VLLM_MODEL || 'microsoft/git-base';
            console.log('[VL LM] Service initialized successfully with model:', this.model);
        } else {
            this.hf = null;
            this.isAvailable = false;
            console.log('[VL LM] API key not configured, service unavailable');
        }
    }

    /**
     * Process command with VL LM
     */
    async processCommand(userInput, clientInfo, context = {}) {
        try {
            if (!this.isAvailable) {
                throw new Error('VL LM API key not configured');
            }

            console.log('[VL LM] Processing command:', userInput);
            
            const prompt = this.buildCommandPrompt(userInput, clientInfo, context);
            
            const response = await this.hf.textGeneration({
                model: this.model,
                inputs: prompt,
                parameters: {
                    max_new_tokens: 1000,
                    temperature: 0.7,
                    return_full_text: false
                }
            });

            const generatedText = response.generated_text || '';
            const commandResult = this.parseCommandResponse(generatedText);
            
            // Ensure consistent response format matching Gemini
            return {
                success: true,
                data: {
                    command: commandResult.command,
                    type: commandResult.type || 'powershell',
                    timeout: commandResult.timeout || 300,
                    explanation: commandResult.explanation || 'Command generated by VL LM',
                    safety_level: commandResult.safety_level || 'safe',
                    alternatives: commandResult.alternatives || [],
                    aiProcessed: true,
                    model: this.model,
                    provider: 'vllm',
                    timestamp: new Date().toISOString()
                }
            };
        } catch (error) {
            console.error('[VL LM] Error processing command:', error);
            throw error;
        }
    }

    /**
     * Build command prompt for VL LM
     */
    buildCommandPrompt(userInput, clientInfo, context) {
        const platform = clientInfo.platform || 'unknown';
        const systemInfo = clientInfo.systemInfo || {};
        
        return `You are an expert system administrator AI. Generate reliable commands based on user intent.

User Request: "${userInput}"

Target Client:
- Platform: ${platform}
- Computer Name: ${systemInfo.ComputerName || 'Unknown'}
- OS Version: ${systemInfo.OSVersion || 'Unknown'}
- Architecture: ${systemInfo.Is64BitOperatingSystem === 'True' ? '64-bit' : '32-bit'}

Respond with JSON:
{
  "command": "powershell_command_here",
  "type": "powershell",
  "timeout": 300,
  "explanation": "brief explanation",
  "safety_level": "safe|moderate|risky",
  "alternatives": ["alternative1", "alternative2"]
}`;
    }

    /**
     * Parse command response from VL LM
     */
    parseCommandResponse(response) {
        try {
            const jsonMatch = response.match(/\{[\s\S]*\}/);
            if (jsonMatch) {
                return JSON.parse(jsonMatch[0]);
            }
            throw new Error('No JSON found in response');
        } catch (error) {
            console.error('[VL LM] Error parsing response:', error);
            // Fallback response
            return {
                command: `echo "VL LM processing failed: ${error.message}"`,
                type: 'powershell',
                timeout: 30,
                explanation: 'VL LM response parsing failed',
                safety_level: 'safe',
                alternatives: []
            };
        }
    }

    /**
     * Handle errors with VL LM
     */
    async handleError(error, originalCommand, clientInfo, retryCount = 0) {
        try {
            if (!this.isAvailable) {
                throw new Error('VL LM not available');
            }

            const prompt = `Fix this command error:

Error: ${error.message}
Original Command: ${originalCommand.command}
Client Platform: ${clientInfo.platform}
Retry Count: ${retryCount}

Respond with JSON:
{
  "fixed_command": "improved_command",
  "explanation": "why_this_will_work",
  "changes_made": ["change1", "change2"]
}`;

            const response = await this.hf.textGeneration({
                model: this.model,
                inputs: prompt,
                parameters: {
                    max_new_tokens: 500,
                    temperature: 0.5
                }
            });

            const generatedText = response.generated_text || '';
            const jsonMatch = generatedText.match(/\{[\s\S]*\}/);
            
            if (jsonMatch) {
                const fix = JSON.parse(jsonMatch[0]);
                return {
                    success: true,
                    data: {
                        command: fix.fixed_command,
                        explanation: fix.explanation,
                        changes_made: fix.changes_made,
                        aiProcessed: true,
                        retryCount: retryCount + 1,
                        provider: 'vllm'
                    }
                };
            }
            
            throw new Error('Invalid response format');
        } catch (error) {
            console.error('[VL LM] Error handling failed:', error);
            throw error;
        }
    }

    /**
     * Test VL LM connection
     */
    async testConnection() {
        try {
            if (!this.isAvailable) {
                return false;
            }
            
            const response = await this.hf.textGeneration({
                model: this.model,
                inputs: 'test',
                parameters: {
                    max_new_tokens: 5
                }
            });
            
            return !!response;
        } catch (error) {
            console.error('[VL LM] Connection test failed:', error);
            return false;
        }
    }
}

module.exports = VLLMService;
