const { HfInference } = require('@huggingface/inference');

/**
 * VL LM (Vision Language Model) Service
 * Fast backup LLM for command processing when Gemini is unavailable or after training
 */
class VLLMService {
    constructor() {
        this.hf = null;
        this.isAvailable = false;
        this.isTrained = false;
        this.modelName = process.env.VLLM_MODEL_NAME || 'microsoft/DialoGPT-large';
        
        // Initialize Hugging Face client for VL LM
        const apiKey = process.env.HUGGINGFACE_API_KEY || process.env.VLLM_API_KEY;
        if (apiKey && apiKey !== 'your-huggingface-api-key-here') {
            this.hf = new HfInference(apiKey);
            this.isAvailable = true;
            console.log('[VLLM] VL LM service initialized successfully');
            console.log('[VLLM] Model:', this.modelName);
            
            // Check if model is trained (can be set via environment variable)
            this.isTrained = process.env.VLLM_TRAINED === 'true' || false;
            if (this.isTrained) {
                console.log('[VLLM] Model is marked as trained - ready for full-time usage');
            } else {
                console.log('[VLLM] Model is not yet trained - will use Gemini as primary');
            }
        } else {
            console.log('[VLLM] VL LM API key not configured, using fallback mode');
            this.isAvailable = false;
        }
    }

    /**
     * Process command using VL LM
     */
    async processCommand(userInput, clientInfo, context = {}) {
        try {
            if (!this.isAvailable) {
                console.log('[VLLM] VL LM not available, falling back');
                return await this.fallbackProcessing(userInput, clientInfo, context);
            }

            const platform = clientInfo.platform || 'unknown';
            const systemInfo = clientInfo.systemInfo || {};

            // Build prompt for VL LM
            const prompt = this.buildVLLMPrompt(userInput, clientInfo, context);

            console.log('[VLLM] Processing command with VL LM:', userInput);
            
            // Use text generation
            const response = await this.hf.textGeneration({
                model: this.modelName,
                inputs: prompt,
                parameters: {
                    max_new_tokens: 1000,
                    temperature: 0.7,
                    top_p: 0.9,
                    return_full_text: false
                }
            });

            const generatedText = response.generated_text || '';
            console.log('[VLLM] Generated text:', generatedText);

            // Parse response
            const parsedResponse = this.parseResponse(generatedText, userInput, clientInfo);

            return {
                success: true,
                data: {
                    command: parsedResponse.command,
                    type: parsedResponse.type || 'powershell',
                    timeout: parsedResponse.timeout || 300,
                    explanation: parsedResponse.explanation || 'Command generated by VL LM',
                    safety_level: parsedResponse.safety_level || 'safe',
                    alternatives: parsedResponse.alternatives || [],
                    aiProcessed: true,
                    model: this.modelName,
                    provider: 'vllm',
                    timestamp: new Date().toISOString()
                }
            };

        } catch (error) {
            console.error('[VLLM] Error processing command:', error);
            return await this.fallbackProcessing(userInput, clientInfo, context);
        }
    }

    /**
     * Build prompt for VL LM
     */
    buildVLLMPrompt(userInput, clientInfo, context) {
        const platform = clientInfo.platform || 'unknown';
        const systemInfo = clientInfo.systemInfo || {};
        
        return `You are an expert system administrator AI. Generate a reliable command for the following request.

User Request: "${userInput}"
Platform: ${platform}
Computer Name: ${systemInfo.ComputerName || 'Unknown'}
OS Version: ${systemInfo.OSVersion || 'Unknown'}
Architecture: ${systemInfo.Is64BitOperatingSystem === 'True' ? '64-bit' : '32-bit'}

Generate a JSON response:
{
  "command": "command_to_execute",
  "type": "powershell|cmd|bash",
  "timeout": 300,
  "explanation": "what_this_command_does",
  "safety_level": "safe|moderate|risky",
  "alternatives": ["alternative_command_1", "alternative_command_2"]
}

Response:`;
    }

    /**
     * Parse VL LM response
     */
    parseResponse(generatedText, userInput, clientInfo) {
        try {
            // Try to extract JSON from generated text
            const jsonMatch = generatedText.match(/\{[\s\S]*\}/);
            if (jsonMatch) {
                const parsed = JSON.parse(jsonMatch[0]);
                if (parsed.command) {
                    return parsed;
                }
            }
        } catch (error) {
            console.error('[VLLM] Error parsing response:', error);
        }

        // Fallback: Generate simple command
        return this.generateSimpleCommand(userInput, clientInfo);
    }

    /**
     * Generate simple command as fallback
     */
    generateSimpleCommand(userInput, clientInfo) {
        const platform = clientInfo.platform || 'windows';
        const input = userInput.toLowerCase();
        
        let command = '';
        let type = platform === 'windows' ? 'powershell' : 'bash';
        
        if (input.includes('system info') || input.includes('computer info')) {
            command = platform === 'windows'
                ? 'Get-ComputerInfo | Select-Object WindowsProductName, WindowsVersion, TotalPhysicalMemory'
                : 'uname -a && free -h && df -h';
        } else if (input.includes('list') && input.includes('process')) {
            command = platform === 'windows'
                ? 'Get-Process | Select-Object Name, Id, CPU | Sort-Object CPU -Descending'
                : 'ps aux | head -20';
        } else if (input.includes('memory') || input.includes('ram')) {
            command = platform === 'windows'
                ? 'Get-WmiObject -Class Win32_PhysicalMemory | Measure-Object -Property Capacity -Sum'
                : 'free -h';
        } else {
            command = userInput;
        }

        return {
            command: command,
            type: type,
            timeout: 300,
            explanation: 'Command generated by VL LM fallback',
            safety_level: 'safe',
            alternatives: []
        };
    }

    /**
     * Fallback processing
     */
    async fallbackProcessing(userInput, clientInfo, context) {
        console.log('[VLLM] Using fallback processing');
        const simpleCommand = this.generateSimpleCommand(userInput, clientInfo);
        
        return {
            success: true,
            data: {
                ...simpleCommand,
                aiProcessed: false,
                model: 'fallback',
                provider: 'fallback',
                timestamp: new Date().toISOString()
            }
        };
    }

    /**
     * Mark model as trained (for switching from Gemini to VL LM)
     */
    markAsTrained() {
        this.isTrained = true;
        process.env.VLLM_TRAINED = 'true';
        console.log('[VLLM] Model marked as trained - ready for full-time usage');
    }

    /**
     * Check if model is trained and ready
     */
    isModelTrained() {
        return this.isTrained && this.isAvailable;
    }

    /**
     * Test VL LM connection
     */
    async testConnection() {
        try {
            if (!this.isAvailable) {
                return false;
            }

            const response = await this.hf.textGeneration({
                model: this.modelName,
                inputs: 'Test',
                parameters: {
                    max_new_tokens: 10
                }
            });

            return !!response.generated_text;
        } catch (error) {
            console.error('[VLLM] Connection test failed:', error);
            return false;
        }
    }
}

module.exports = VLLMService;
